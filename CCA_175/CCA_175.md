Tips : Documents\Teradata\RampUP\video\01. Introduction

Avoid typing the directory names, just copy from the questions and paste in your program. Otherwise, your output will go on different directory and you will not get credit even for the correct answer.

You should know to increase the font size of window. Ctrl+ for browser and ctrl+shift+ for terminal. I must say you will not able to differentiate dot(.) and(*) and i guarantee that you need to know how to increase font size.

You have to memorize some of the commands, configuration, compression parameters. Better to memorize those stuff which you don’t want to search in documentation as this will KILL the time.

the exam allows you to access some documentation online, namely the list at the bottom of the exam home page. Familiarize yourself with the necessary sections there (Sqoop, Spark, Flume, Python, and Scala).

Duration: 120 Minutes

Passing Score : 7 out of 9 Questions need be solved correctly.

Number of Questions: Total 9 questions (1 Sqoop Import, 1 Sqoop Export , 7 Spark )

I have been asked 2 sqoop, 1 hive and 6 spark questions. Sqoop questions are generally easy,command based and can be prepared in short time. Get the sqoop ready in order to get 2 questions credits very easily.

Use Zooming (Ctrl + ) and Copy — Paste ( ctrl+shift+c and ctrl+shift+v) Commands for better productivity.

You should know to increase the font size of window. Ctrl+ for browser and ctrl+shift+ for terminal. I must say you will not able to differentiate dot(.) and(*) and i guarantee that you need to know how to increase font size.

You can take exam either in Spark 2.3 or Spark 1.6 using spark-shell or spark2-shell respectively.

No need to save the code anywhere, as cloudera doesn’t evaluate the code. It only checks the output requirements like data, folder path, file format and compression technique used.

Plan :
http://arun-teaches-u-tech.blogspot.com/p/certification-preparation-plan.html

This one seems better -->
https://www.alpha-epsilon.de/cca175/2017/07/16/preparing-for-the-cloudera-exam-cca175-spark-and-hadoop-developer/ 

https://andriymz.github.io/certifications/cca175-feedback/

https://github.com/topics/cca175

Practise exam : 
    http://nn02.itversity.com/cca175/
	https://github.com/Prakash-Ponnusamy1/CCA175_Master_Preparation
Compatibility Check :

    https://www.examslocal.com/ScheduleExam/Home/CompatibilityCheck

Tips :
   https://medium.com/@chappidisurendra/all-you-need-to-know-about-cloudera-cca-175-exam-a0afaa018958 


Sqoop :
https://hadoopbaseblog.wordpress.com/2017/03/03/sqoop-related-exam-questions-cca-175/

http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.15.0/SqoopUserGuide.html

Spark:

http://spark.apache.org/docs/2.3.0/

Docker Image :
https://medium.com/@dataakkadian/how-to-install-and-running-cloudera-docker-container-on-ubuntu-b7c77f147e03

Check for Docker images at C:\ProgramData\DockerDesktop\vm-data\DockerDesktop.vhdx

Expand the downloaded tar.gz file. There is packaging issue with manual download, untar multiple times to get the final .tar file
Import the docker image:
    docker import cloudera-quickstart-vm-5.13.0-0-beta-docker.tar cloudera/quickstart:latest
	
	docker run --hostname=quickstart.cloudera --privileged=true -t -i -p 7180:7180 -p 80:8089 -p 8888:8888 -p 7187:7187 -p 8088:8088 -p 19888:19888 --name cca175 cloudera/quickstart /usr/bin/docker-quickstart
	
	OR 
	
	docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname=quickstart.cloudera --privileged=true -t -i -v $(pwd):/zaid --publish-all=true -p8888 -p8088 --name cca175 cloudera/quickstart /usr/bin/docker-quickstart



Cloudera Manager is not started by default. To see options for starting Cloudera Manager, run the following command:

/home/cloudera/cloudera-manager

You can look up the interface to which it binds and the port number it maps to using the following command:

docker port [CONTAINER HASH] [GUEST PORT]

And to see if we are running Docker CE with the minimum configuration we use this command.

docker stats [CONTAINER ID]

you can connect to the shell later using the following command:

docker attach [CONTAINER ID]   -- same session

docker exec -it  [CONTAINER ID] /bin/bash -- differnt session
-------------------------------------------------------------------

# Sqoop

From the command line, access a MySQL on localhost via

 mysql -h localhost -u root -pcloudera

JDBC connection parameters for the gateway.

hostname -i  
or
hostname


show databases;  -- list all available databases
use retail_db;   -- switch to this database
show tables;
select * from customers limit 10;  -- query the table

//IGNORE
//Sqoop needs permissions to access the tables. You grant these permissions from MySQL like so:
//GRANT ALL PRIVILEGES ON <database_name>.* to ''@'localhost';

##Import from MySQL to HDFS

The core command here is sqoop import, along with a lot of parameters. This is an extensive example command:

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username root \
  --password cloudera \
  --table products \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
  --target-dir /home/cloudera/sqoop/problem1/products \
  --as-avrodatafile;

Additionally to importing a table, this command compresses the HDFS file using the snappy codec (alternatives to .SnappyCodec are BZip2Codec, GzipCodec, and DefaultCodec).

available codecs:

bzip2           .bz2            org.apache.hadoop.io.compress.BZip2Codec
default         .deflate        org.apache.hadoop.io.compress.DefaultCodec
deflate         .deflate        org.apache.hadoop.io.compress.DeflateCodec
gzip            .gz             org.apache.hadoop.io.compress.GzipCodec
lz4             .lz4            org.apache.hadoop.io.compress.Lz4Codec
snappy          .snappy         org.apache.hadoop.io.compress.SnappyCodec

***NOTE:*** The codec full name can also be figured out from file core-site.xml at /etc/hadoop/conf


It also imports the file as an Avro file (instead of the default, a text file).

Check files at 
hadoop fs -ls /home/cloudera/sqoop/problem1/products

Hadoop RM : http://localhost:8088/cluster
Hue : http://localhost:8888 ( cloudera,cloudera )

-------------------------------------------------------------------
hadoop checknative -a 




//pyspark -shell
  
    pyspark --master yarn --jars /home/cloudera/Downloads/spark-avro_2.10-2.0.1.jar
	bin/pyspark --packages org.apache.spark:spark-avro_2.11:2.4.0
	
	
//Spark SQL
spark-sql --master yarn --name spark_load_test -e "select * from retail.dgentab limit 2; select * from retail.dgentab limit 2;"

spark-submit --master yarn test.py	
	
// For auto complete
import rlcompleter, readline
readline.parse_and_bind('tab: complete')	

To make sure this setting is persistent every time when pyspark is launched. Below was done.

Step1: Add below to ~/.pythonrc

import rlcompleter, readline
readline.parse_and_bind('tab: complete')

Step2: Add below to ~/.bash_profile
export PYTHONSTARTUP="$HOME/.pythonrc"


PYSPARK_PYTHON=/home/cloudera/Python-3.6.2 in spark-env.sh

from_unixtime(unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss'),'yyyy/MM/dd')

-------------------------------------------------------------------
$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-avro_2.11:4.0.0
./bin/spark-shell --packages org.apache.spark:spark-avro_2.12:2.4.4


# Creates a DataFrame from a directory
df = sqlContext.read.format("com.databricks.spark.avro").load("input_dir")

#  Saves the subset of the Avro records read in
df.where("age > 5").write.format("com.databricks.spark.avro").save("output_dir")


-------------------------------------------------------------------------

### Supported SQL functions 

https://spark.apache.org/docs/2.3.0/api/sql/

Actual files are at : 

spark-2.2.1-bin-hadoop2.7\python\pyspark\sql\functin

( https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF )

https://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4
https://alvinhenrick.com/2017/05/16/apache-spark-analytical-window-functions/
http://dwgeek.com/spark-sql-analytic-functions-and-examples.html/

#### String Functions

	1. concat(string|binary A, string|binary B...) :
		
	2. concat_ws(string SEP, string A, string B...) :
		
	3. encode(string src, string charset) :
		
	4. format_number(number x, int d)
		
	5. length(string A)
		
	6. lower(string A)
		
	7. lpad(string str, int len, string pad)
		
	8. repeat(string str, int n)
		
	9. replace(string A, string OLD, string NEW)
		Json element syntax : "transFunc":"encode({0} ,'UTF-8')"
		--> pyspark.sql.utils.AnalysisException: "Undefined function: 'replace'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 5"

	10. reverse(string A)
		
	11. substr(string|binary A, int start) / substr(string|binary A, int start, int len) 
		
	12. substring(string|binary A, int start) / substring(string|binary A, int start, int len)
	
	13. split
		split(str, regex) - Splits str around occurrences that match regex.

		Examples:

		> SELECT split('oneAtwoBthreeC', '[ABC]');
		 ["one","two","three",""]
		
	13. trim(string A)
		
	14. upper(string A) 
		
	16. ucase(string A)
	
	
	17. like
	
	***18. rlike ***
	str rlike regexp - Returns true if str matches regexp, or false otherwise.

		Arguments:

		str - a string expression
		regexp - a string expression. The pattern string should be a Java regular expression.

		Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL parser. For example, to match "\abc", a regular expression for regexp can be "^\abc$".

		There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to fallback to the Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the regexp that can match "\abc" is "^\abc$".

		Examples:

		When spark.sql.parser.escapedStringLiterals is disabled (default).
		> SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\\Users.*'
		true

		When spark.sql.parser.escapedStringLiterals is enabled.
		> SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\Users.*'
		true
			
	19. replace
		replace(str, search[, replace]) - Replaces all occurrences of search with replace.

		Arguments:

		str - a string expression
		search - a string expression. If search is not found in str, str is returned unchanged.
		replace - a string expression. If replace is not specified or is an empty string, nothing replaces the string that is removed from str.
		Examples:

		> SELECT replace('ABCabc', 'abc', 'DEF');
		 ABCDEF
	
	19. regexp_extract
		regexp_extract(str, regexp[, idx]) - Extracts a group that matches regexp.

		Examples:

		> SELECT regexp_extract('100-200', '(\d+)-(\d+)', 1);
		 100
	20. regexp_replace
		regexp_replace(str, regexp, rep) - Replaces all substrings of str that match regexp with rep.

		Examples:

		> SELECT regexp_replace('100-200', '(\d+)', 'num');
		 num-num
 
	19. parse_url
		parse_url(url, partToExtract[, key]) - Extracts a part from a URL.

		Examples:

		> SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST')
		 spark.apache.org
		> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY')
		 query=1
		> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query')
		 1
		

#### Conditional Functions
	1. nvl(T value, T default_value)
		
	2. COALESCE(T v1, T v2, ...)
		


#### Date Functions
	1. from_unixtime(bigint unixtime[, string format])
		syntax : "from_unixtime({0} ,'yyyy/MM/dd')"
	2. unix_timestamp()
		
	3. unix_timestamp(string date)

	4. unix_timestamp(string date, string pattern)
		syntax : "unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss')"
	5. to_date(string timestamp)
		syntax : "to_date(current_timestamp())"
	6. current_date
		syntax : "current_date()"
	7. current_timestamp
		syntax : "current_timestamp()"
		
	8. to_timestamp
		to_timestamp(timestamp[, fmt]) - Parses the timestamp expression with the fmt expression to a timestamp. Returns null with invalid input. By default, it follows casting rules to a timestamp if the fmt is omitted.

		Examples:

		> SELECT to_timestamp('2016-12-31 00:12:00');
		 2016-12-31 00:12:00
		> SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');
		 2016-12-31 00:00:00	

	9.to_date(col, format=None)
 	Converts a Column of StringType or TimestampType into DateType using the optionally specified format. By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted (equivalent to col.cast("date")).	
		to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')
		
***Note:* To convert a date string of one format to another use following **

"from_unixtime(unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss'),'yyyy/MM/dd')"

***Note:* To convert a date string of one format to Date type **
"to_date(_c0,'yyyyMMdd') as dt"


#### Mathematical Functions
	1. round(DOUBLE a)
		
	2. round(DOUBLE a, INT d)
	
	3. format_number
		format_number(expr1, expr2) - Formats the number expr1 like '#,###,###.##', rounded to expr2 decimal places. If expr2 is 0, the result has no decimal point or fractional part. 
		
		SELECT format_number(12332.123456, 4);
		12,332.1235
		
	3. floor(DOUBLE a)
		
	4. ceil(DOUBLE a)
		
	5. ceiling(DOUBLE a)
		
###  Window  Functions ( http://www.mysqltutorial.org/mysql-window-functions/ )

	1.lag
		lag(input[, offset[, default]]) - Returns the value of input at the offsetth row ***"before the current row"*** in the window. 
	
	2.lead
		lead(input[, offset[, default]]) - Returns the value of input at the offsetth row ***"after the current row"*** in the window.
		
	3.first_value
	first_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values.	
	
	4.last_value
	last_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values.
		Note : order by doesnt work in last_value clause

#### aggregate function using Over	
	
	5.Count
	It returns the count of all the values for the expression written in the over clause.
		Note: if we add order by then the count does running count
	
	6.Sum
	It returns the sum of all the values for the expression written in the over clause.
		Note: if we add order by in over clause we get a running sum.
		
	7.Min
	It returns the minimum value of the column for the rows in that over clause.	
	
	8.Max
	It returns the maximum value of the column for the rows in that over clause. 
	
	9.AVG
	It returns the average value of the column for the rows that over clause returns.
	
#### Analytic functions (notice the parantheses)
	10.rank()
	The rank function will return the rank of the values as per the result set of the over clause. If two values are same then it will give the same rank to those 2 values and then for the next value, the sub-sequent rank will be skipped.
	
	***Note :*** it is used as functions i.e rank() and order by clause is also used.
	
	11.Dense_rank()
	It is same as the rank() function but the difference is if any duplicate value is present then the rank will not be skipped for the subsequent rows. Each unique value will get the ranks in a sequence.
	
	12.row_number()
	Row number will return the continuous sequence of numbers for all the rows of the result set of the over clause.
	
		colValue|row_number  | rank | dense_rank | percent_rank | cume_dist | ntile   
		--------+------------+------+------------+--------------+-----------+-------
			1	|		   1 |    1 |          1 |            0 |       0.2 |     1 
			2	|	       2 |    2 |          2 |         0.25 |       0.6 |     1 
			2	|	       3 |    2 |          2 |         0.25 |       0.6 |     2 
			3	|	       4 |    4 |          3 |         0.75 |       0.8 |     3 
			4	|	       5 |    5 |          4 |            1 |         1 |     4 
	
	13.cume_dist()
	Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row.
	http://www.mysqltutorial.org/mysql-window-functions/mysql-cume_dist-function/
	
	14.Percent_rank()
	It returns the percentage rank of each row within the result set of over clause. Percent_rank is calculated in accordance with the rank of the row and the calculation is as follows (rank-1)/(total_rows_in_group – 1). If the result set has only one row then the percent_rank will be 0.
	
	Note:
	Definition:
	cume_dist computes the fraction of partition rows ***that are less than or equal to the current row and its peers***, while percent_rank computes the fraction of partition rows ***that are less than the current row, assuming the current row does not exist in the partition.***
	
	Method to calculate:
		PERCENT_RANK = (RANK – 1)/(COUNT -1)
		CUME_DIST = RANK/COUNT
		
	Meaning of the method:
	x | percent_rank | cume_dist
	---+--------------+-----------
	0 |         0.00 |      0.14
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	2 |         1.00 |      1.00
	
	If there are 7 scores and the PERCENT_RANK is 0.17 , that means that the score is higher than 0.17% of scores. If the CUME_DIST is 0.17 , that means that the score is the 0.17 th one in the list.	
	OR 
	If I got a score of 1, is saying that a ratio of 0.17 rows are less than mine (PERCENT_RANK), or that 0.86 are equal or less than mine (cume_dist)
	
	
	15.Ntile()
	It returns the bucket number of the particular value. For suppose if you say Ntile(5) then it will create 5 buckets based on the result set of the over clause after that it will place the first 20% of the records in the 1st bucket and so on till 5th bucket.


#### Conditional Statement

	1. When
	
	2. if 
	
	3.

	
#### Misc Functions

	1. cast
	cast(expr AS type) - Casts the value expr to the target data type type.

	Examples:

	> SELECT cast('10' as int);
	 10
	 
	2. uuid
		uuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.

		Examples:

		> SELECT uuid();
		 46707d92-02f4-4817-8116-a4c3b23e6266
	
	
	4. monotonically_increasing_id

	3. randomUUID
	java_method('java.util.UUID', 'randomUUID');
	c33fb387-8500-4bfa-81d2-6e0e3e930df2
	
	4. reflect
	reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.
	SELECT reflect('java.util.UUID', 'randomUUID');
	c33fb387-8500-4bfa-81d2-6e0e3e930df2
	

	
-----TODO---
explode_outer
inline_outer
from_json
if
when


lag
lead
last
rank

skewness
skewness(expr) - Returns the skewness value calculated from values of a group.	


Save as avro,sequence,orc,
Export Sqoop
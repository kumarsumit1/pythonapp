Tips : Documents\Teradata\RampUP\video\01. 



## Introduction

Duration: 120 Minutes

Passing Score : 7 out of 9 Questions need be solved correctly.

Avoid typing the directory names, just copy from the questions and paste in your program. 

You should know to increase the font size of window. Ctrl+ for browser and ctrl+shift+ for terminal. 
I must say you will not able to differentiate dot(.) and(*) and i guarantee that you need to know how to increase font size.

You have to memorize some of the commands, configuration, compression parameters. 
Better to memorize those stuff which you don’t want to search in documentation as this will KILL the time.


No need to save the code anywhere, as cloudera doesn’t evaluate the code. 
It only checks the output requirements like data, folder path, file format and compression technique used.

Plan :

	https://medium.com/@itversity/cca-175-resources-tips-and-techniques-c1cfdaf1ddd
	
	https://proedu.co/courses/cca-175-spark-and-hadoop-developer-certification/  ( free course )

	Logical breakup of questions :

	https://ajpatel-bigdata.medium.com/cca-spark-and-hadoop-developer-mock-exam-practice-cca175-ee0bcf8ed3a4




	https://github.com/topics/cca175

Practise exam : 
    	
	https://github.com/Prakash-Ponnusamy1/CCA175_Master_Preparation
Compatibility Check :  
	https://university.cloudera.com/user/learning/enrollments 
    https://www.examslocal.com/ScheduleExam/Home  

    https://www.examslocal.com/ScheduleExam/Home/CompatibilityCheck

Tips :
   1. Use Browser as notepad : data:text/html, <html contenteditable>
                               Or Developer Options -> Sources -> Snippets  ( Chorme )
							   Or Developer Options -> Style Editor  ( Firefox )
							   Or Google translate
   2. Key Shortcuts :
		Increasing browser font : Ctrl+ 
		Increasing terminal font : ctrl+shift+
		Terminal copy paste : CTRL+SHIFT+C and CTRL+SHIFT+V  OR CTRL+INSERT to copy and SHIFT+INSERT to paste
							  To summarize, you copy from Browser or Text Editor with CTRL+C and paste in Terminal with CTRL+SHIFT+V or SHIFT+INSERT
   
   3. To work with Python 3.x: 
			In an open terminal type 
			 export PYTHONIOENCODING=utf8
			 export PYSPARK_PYTHON=python3  and hit enter
			 
   4. Know about spark and scala version :
			spark-shell --version
			pyspark --version
			pyspark --help
   4. To execute commands on OS without exitting pyspark session .
      >>> import os
      >>> os.system("hdfs dfs -ls -R /verulam_blue/test1")
      >>> os.system("hdfs dfs -cat /verulam_blue/tests/data/tab_text/hr/part-00000-49651445-1cc9-487b-8f1f-2a2e3dd0b55f-c000.txt.gz | more")
      >>> os.system("hdfs dfs -tail /verulam_blue/tests/data/tab_text/hr/part-00000-49651445-1cc9-487b-8f1f-2a2e3dd0b55f-c000.txt.gz")		 

   5. To check for warehouse dir 
      spark.sparkContext.getConf().get('spark.sql.warehouse.dir')
	  
   6.  Spark Doc 

		http://spark.apache.org/docs/2.4.0/
		
   7. HDFS commands 
		HDFS commands (mostly ls and tail)
			import os
			os.system("hdfs dfs -ls /public/data/output/sol2/")
			os.system("hdfs dfs -cat /public/data/output/sol2/part-00000-695221b2-113d-483c-9405-dea6d1c759f9-c000.txt | more")
			
   8. Read from local folders 
       sc.textFile("file:///path to the file/")   
	   
   9. Analytical Function : 
		https://www.youtube.com/watch?v=JqKACJBr7hE

   https://medium.com/@chappidisurendra/all-you-need-to-know-about-cloudera-cca-175-exam-a0afaa018958 
   
   9. Helpful DF methods :
      head(number of rows)
	  count()  //avoid 
	  


Note : 

1. write and read doesn't have () parantheses ( Error : object is not callable )





### coupon code :
please use coupon code "ClouderaCert_20" when checking out to receive 20% off your certification exam. 
we are providing a 50% discount for your second attempt using the coupon code "SecondShot".




### Docker Image :
https://medium.com/@dataakkadian/how-to-install-and-running-cloudera-docker-container-on-ubuntu-b7c77f147e03

Check for Docker images at C:\ProgramData\DockerDesktop\vm-data\DockerDesktop.vhdx

Expand the downloaded tar.gz file. There is packaging issue with manual download, untar multiple times to get the final .tar file
Import the docker image:
    docker import cloudera-quickstart-vm-5.13.0-0-beta-docker.tar cloudera/quickstart:latest
	
	docker run --hostname=quickstart.cloudera --privileged=true -t -i -p 7180:7180 -p 80:8089 -p 8888:8888 -p 7187:7187 -p 8088:8088 -p 19888:19888 --name cca175 cloudera/quickstart /usr/bin/docker-quickstart
	
	OR 
	
	docker run -m 4G --memory-reservation 2G --memory-swap 8G --hostname=quickstart.cloudera --privileged=true -t -i -v $(pwd):/zaid --publish-all=true -p8888 -p8088 --name cca175 cloudera/quickstart /usr/bin/docker-quickstart



Cloudera Manager is not started by default. To see options for starting Cloudera Manager, run the following command:

/home/cloudera/cloudera-manager

You can look up the interface to which it binds and the port number it maps to using the following command:

docker port [CONTAINER HASH] [GUEST PORT]

And to see if we are running Docker CE with the minimum configuration we use this command.

docker stats [CONTAINER ID]

you can connect to the shell later using the following command:

docker attach [CONTAINER ID]   -- same session

docker exec -it  [CONTAINER ID] /bin/bash -- differnt session
-------------------------------------------------------------------

# Sqoop

From the command line, access a MySQL on localhost via

 mysql -h localhost -u root -pcloudera

JDBC connection parameters for the gateway.

hostname -i  
or
hostname


show databases;  -- list all available databases
use retail_db;   -- switch to this database
show tables;
select * from customers limit 10;  -- query the table

//IGNORE
//Sqoop needs permissions to access the tables. You grant these permissions from MySQL like so:
//GRANT ALL PRIVILEGES ON <database_name>.* to ''@'localhost';

##Import from MySQL to HDFS

The core command here is sqoop import, along with a lot of parameters. This is an extensive example command:

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username root \
  --password cloudera \
  --table products \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
  --target-dir /home/cloudera/sqoop/problem1/products \
  --as-avrodatafile;

Additionally to importing a table, this command compresses the HDFS file using the snappy codec (alternatives to .SnappyCodec are BZip2Codec, GzipCodec, and DefaultCodec).

available codecs:

bzip2           .bz2            org.apache.hadoop.io.compress.BZip2Codec
default         .deflate        org.apache.hadoop.io.compress.DefaultCodec
deflate         .deflate        org.apache.hadoop.io.compress.DeflateCodec
gzip            .gz             org.apache.hadoop.io.compress.GzipCodec
lz4             .lz4            org.apache.hadoop.io.compress.Lz4Codec
snappy          .snappy         org.apache.hadoop.io.compress.SnappyCodec

***NOTE:*** The codec full name can also be figured out from file core-site.xml at /etc/hadoop/conf


It also imports the file as an Avro file (instead of the default, a text file).

Check files at 
hadoop fs -ls /home/cloudera/sqoop/problem1/products

Hadoop RM : http://localhost:8088/cluster
Hue : http://localhost:8888 ( cloudera,cloudera )

-------------------------------------------------------------------
hadoop checknative -a 

To check number of nodes 
yarn node -list -all

hdfs dfsadmin -report 


//pyspark -shell
  
    pyspark --master yarn --jars /home/cloudera/Downloads/spark-avro_2.10-2.0.1.jar
	bin/pyspark --master yarn --packages org.apache.spark:spark-avro_2.11:2.4.0
	
	
//Spark SQL
spark-sql --master yarn --name spark_load_test -e "select * from retail.dgentab limit 2; select * from retail.dgentab limit 2;"

spark-submit --master yarn test.py	
	
// For auto complete
import rlcompleter, readline
readline.parse_and_bind('tab: complete')	

To make sure this setting is persistent every time when pyspark is launched. Below was done.

Step1: Add below to ~/.pythonrc

import rlcompleter, readline
readline.parse_and_bind('tab: complete')

Step2: Add below to ~/.bash_profile
export PYTHONSTARTUP="$HOME/.pythonrc"


PYSPARK_PYTHON=/home/cloudera/Python-3.6.2 in spark-env.sh

from_unixtime(unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss'),'yyyy/MM/dd')

-------------------------------------------------------------------
$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-avro_2.11:4.0.0
./bin/spark-shell --packages org.apache.spark:spark-avro_2.12:2.4.4


# Creates a DataFrame from a directory
df = sqlContext.read.format("com.databricks.spark.avro").load("input_dir")

#  Saves the subset of the Avro records read in
df.where("age > 5").write.format("com.databricks.spark.avro").save("output_dir")


-------------------------------------------------------------------------

### Supported SQL functions 

https://spark.apache.org/docs/2.4.0/api/sql/

Actual files are at : 

spark-2.2.1-bin-hadoop2.7\python\pyspark\sql\functin

( https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF )

https://medium.com/jbennetcodes/how-to-get-rid-of-loops-and-use-window-functions-in-pandas-or-spark-sql-907f274850e4
https://alvinhenrick.com/2017/05/16/apache-spark-analytical-window-functions/
http://dwgeek.com/spark-sql-analytic-functions-and-examples.html/

#### String Functions

	1. concat(string|binary A, string|binary B...) :
		
	2. concat_ws(string SEP, string A, string B...) :
		
	3. encode(string src, string charset) :
		
	4. format_number(number x, int d)
		
	5. length(string A)
		
	6. lower(string A)
		
	7. lpad(string str, int len, string pad)
		
	8. repeat(string str, int n)
		
	9. replace(string A, string OLD, string NEW)
		Json element syntax : "transFunc":"encode({0} ,'UTF-8')"
		--> pyspark.sql.utils.AnalysisException: "Undefined function: 'replace'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 5"

	10. reverse(string A)
		
	11. substr(string|binary A, int start) / substr(string|binary A, int start, int len) 
		
	12. substring(string|binary A, int start) / substring(string|binary A, int start, int len)
	
	13. split
		split(str, regex) - Splits str around occurrences that match regex.

		Examples:

		> SELECT split('oneAtwoBthreeC', '[ABC]');
		 ["one","two","three",""]
		
	13. trim(string A)
		
	14. upper(string A) 
		
	16. ucase(string A)
	
	
	17. like
	
	***18. rlike ***
	str rlike regexp - Returns true if str matches regexp, or false otherwise.

		Arguments:

		str - a string expression
		regexp - a string expression. The pattern string should be a Java regular expression.

		Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL parser. 
		For example, to match "\abc", a regular expression for regexp can be "^\abc$".

		There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to fallback to the Spark 1.6 behavior regarding string literal parsing. For example, if the config is enabled, the regexp that can match "\abc" is "^\abc$".

		Examples:

		When spark.sql.parser.escapedStringLiterals is disabled (default).
		> SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\\Users.*'
		true

		When spark.sql.parser.escapedStringLiterals is enabled.
		> SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\Users.*'
		true
			
	19. replace
		replace(str, search[, replace]) - Replaces all occurrences of search with replace.

		Arguments:

		str - a string expression
		search - a string expression. If search is not found in str, str is returned unchanged.
		replace - a string expression. If replace is not specified or is an empty string, nothing replaces the string that is removed from str.
		Examples:

		> SELECT replace('ABCabc', 'abc', 'DEF');
		 ABCDEF
	
	19. regexp_extract
		regexp_extract(str, regexp[, idx]) - Extracts a group that matches regexp.

		Examples:

		> spark.sql(r"SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1)").show();
		 100
		 
		Note : 
		Use raw string in query i.e r as prefix
		Either use escape literals :
		spark.sql(r"select regexp_extract('123','(\\d+)',1)").show(n=5) 
		
		or "SET spark.sql.parser.escapedStringLiterals=true" and then use
		
		spark.sql(r"select regexp_extract('123','(\d+)',1)").show(n=5) 
		
		
	20. regexp_replace
		regexp_replace(str, regexp, rep) - Replaces all substrings of str that match regexp with rep.

		Examples:

		> SELECT regexp_replace('100-200', '(\d+)', 'num');
		 num-num
 
	19. parse_url
		parse_url(url, partToExtract[, key]) - Extracts a part from a URL.

		Examples:

		> SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST')
		 spark.apache.org
		> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY')
		 query=1
		> SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query')
		 1
		

#### Conditional Functions
	1. nvl(T value, T default_value)
		
	2. COALESCE(T v1, T v2, ...)
		


#### Date Functions
	1. from_unixtime(bigint unixtime[, string format])
		syntax : "from_unixtime({0} ,'yyyy/MM/dd')"
	2. unix_timestamp()
		
	3. unix_timestamp(string date)

	4. unix_timestamp(string date, string pattern)
		syntax : "unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss')"
	5. to_date(string timestamp)
		syntax : "to_date(current_timestamp())"
	6. current_date
		syntax : "current_date()"
	7. current_timestamp
		syntax : "current_timestamp()"
		
	8. to_timestamp
		to_timestamp(timestamp[, fmt]) - Parses the timestamp expression with the fmt expression to a timestamp. Returns null with invalid input. 
		By default, it follows casting rules to a timestamp if the fmt is omitted.

		Examples:

		> SELECT to_timestamp('2016-12-31 00:12:00');
		 2016-12-31 00:12:00
		> SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');
		 2016-12-31 00:00:00	

	9.to_date(col, format=None)
 	Converts a Column of StringType or TimestampType into DateType using the optionally specified format. 
	By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted (equivalent to col.cast("date")).	
		to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')
		
***Note:* To convert a date string of one format to another use following **

"from_unixtime(unix_timestamp({0}, 'yyyy-dd-MM HH:mm:ss'),'yyyy/MM/dd')"

***Note:* To convert a date string of one format to Date type **
"to_date(_c0,'yyyyMMdd') as dt"


#### Mathematical Functions
	1. round(DOUBLE a)
		
	2. round(DOUBLE a, INT d)
	
	3. format_number
		format_number(expr1, expr2) - Formats the number expr1 like '#,###,###.##', rounded to expr2 decimal places. If expr2 is 0, the result has no decimal point or fractional part. 
		
		SELECT format_number(12332.123456, 4);
		12,332.1235
		
	3. floor(DOUBLE a)
		
	4. ceil(DOUBLE a)
		
	5. ceiling(DOUBLE a)
		
###  Window  Functions ( http://www.mysqltutorial.org/mysql-window-functions/ )

	1.lag
		lag(input[, offset[, default]]) - The LAG() function allows access to a value stored in a different row ***above the current row**. 
		
		i.e Values the current row is lagging from 
	
	2.lead
		lead(input[, offset[, default]]) - The LEAD() function allows access to a value stored in a different row ***below the current row**. 
		
		i.e Values the current row is leading from 
		
		
	3.first_value
	first_value(expr[, isIgnoreNull]) - Returns the first value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values.	
	
	4.last_value
	last_value(expr[, isIgnoreNull]) - Returns the last value of expr for a group of rows. If isIgnoreNull is true, returns only non-null values.
		
	Note : order by doesnt work in last_value clause as usual because 
		
		The default windowing clause is "RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW", 
		which in this example means the first row with the same value as that of the current row 
		will always be the last row considered. Altering the windowing clause to 
	    "ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING"
		

		https://medium.com/@tewari.lalit1991/window-functions-in-hive-spark-sql-73ecffa79335
		https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html 

#### aggregate function using Over	
	
	5.Count
	It returns the count of all the values for the expression written in the over clause.
		Note: if we add order by then the count does running count
	
	6.Sum
	It returns the sum of all the values for the expression written in the over clause.
		Note: if we add order by in over clause we get a running sum.
		
	7.Min
	It returns the minimum value of the column for the rows in that over clause.	
	
	8.Max
	It returns the maximum value of the column for the rows in that over clause. 
	
	9.AVG
	It returns the average value of the column for the rows that over clause returns.
	
#### Analytic functions (notice the parantheses)
	10.rank()
	The rank function will return the rank of the values as per the result set of the over clause. If two values are same then it will give the same rank to those 2 values and then for the next value, the sub-sequent rank will be skipped.
	
	***Note :*** it is used as functions i.e rank() and order by clause is also used.
	
	11.Dense_rank()
	It is same as the rank() function but the difference is if any duplicate value is present then the rank will not be skipped for the subsequent rows. Each unique value will get the ranks in a sequence.
	
	12.row_number()
	Row number will return the continuous sequence of numbers for all the rows of the result set of the over clause.
	
		colValue|row_number  | rank | dense_rank | percent_rank | cume_dist | ntile   
		--------+------------+------+------------+--------------+-----------+-------
			1	|		   1 |    1 |          1 |            0 |       0.2 |     1 
			2	|	       2 |    2 |          2 |         0.25 |       0.6 |     1 
			2	|	       3 |    2 |          2 |         0.25 |       0.6 |     2 
			3	|	       4 |    4 |          3 |         0.75 |       0.8 |     3 
			4	|	       5 |    5 |          4 |            1 |         1 |     4 
	
	13.cume_dist()
	Window function: returns the cumulative distribution of values within a window partition, i.e. the fraction of rows that are below the current row.
	http://www.mysqltutorial.org/mysql-window-functions/mysql-cume_dist-function/
	
	14.Percent_rank()
	It returns the percentage rank of each row within the result set of over clause. Percent_rank is calculated in accordance with the rank of the row and the calculation is as follows (rank-1)/(total_rows_in_group – 1). If the result set has only one row then the percent_rank will be 0.
	
	Note:
	Definition:
	cume_dist computes the fraction of partition rows ***that are less than or equal to the current row and its peers***, while percent_rank computes the fraction of partition rows ***that are less than the current row, assuming the current row does not exist in the partition.***
	
	Method to calculate:
		PERCENT_RANK = (RANK – 1)/(COUNT -1)
		CUME_DIST = RANK/COUNT
		
	Meaning of the method:
	x | percent_rank | cume_dist
	---+--------------+-----------
	0 |         0.00 |      0.14
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	1 |         0.17 |      0.86
	2 |         1.00 |      1.00
	
	If there are 7 scores and the PERCENT_RANK is 0.17 , that means that the score is higher than 0.17% of scores. If the CUME_DIST is 0.17 , that means that the score is the 0.17 th one in the list.	
	OR 
	If I got a score of 1, is saying that a ratio of 0.17 rows are less than mine (PERCENT_RANK), or that 0.86 are equal or less than mine (cume_dist)
	
	
	15.Ntile()
	It returns the bucket number of the particular value. For suppose if you say Ntile(5) then it will create 5 buckets based on the result set of the over clause after that it will place the first 20% of the records in the 1st bucket and so on till 5th bucket.


#### Conditional Statement

	1. When
	
	2. if 
	
	3.

	
#### Misc Functions

	1. cast
	cast(expr AS type) - Casts the value expr to the target data type type.

	Examples:

	> SELECT cast('10' as int);
	 10
	 
	2. uuid
		uuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.

		Examples:

		> SELECT uuid();
		 46707d92-02f4-4817-8116-a4c3b23e6266
	
	
	4. monotonically_increasing_id

	3. randomUUID
	java_method('java.util.UUID', 'randomUUID');
	c33fb387-8500-4bfa-81d2-6e0e3e930df2
	
	4. reflect
	reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.
	SELECT reflect('java.util.UUID', 'randomUUID');
	c33fb387-8500-4bfa-81d2-6e0e3e930df2
	
# Spark Catalog Query 


	
-----TODO---
explode_outer
inline_outer
from_json
if
when


lag
lead
last
rank

skewness
skewness(expr) - Returns the skewness value calculated from values of a group.	


Save as avro,sequence,orc,

Each user is given their own CDH6 (currently 6.1.1) cluster pre-loaded with Spark 2.4.

$ wget https://archive.cloudera.com/cm6/6.1.1/cloudera-manager-installer.bin

$ chmod u+x cloudera-manager-installer.bin

$ sudo ./cloudera-manager-installer.bin

https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_61_download.html#cdh_611-download


------------------------- Sample Questions Total 9 Questions ---------------------------
1. Rea csv files in spark 
2. read csv with header
3. read csv with datatype as per file data 
4. Write data in table 
5. write data at a specific location 
6. write data in parquet format 
7. Hadling null, blank, NA, NAN etc 
8. read data in some file format or from hive. For text file practice with different kinds of delimiters, like comma(’,’) or tab (’\t’) or pipe (’|’)
	result = df.select(concat_ws("\t", df.col1, df.col2))
	
	If the exam asks to save in text file format with any delimiter mentioned then I’ll use spark.write.csv() function and save as csv file.
If the exam just asks to save in text file format without any mention about delimiter then I’ll concat the columns with concat_ws function and save using spark.write.text() function.

--------------------------  File formats -----------------------
## Hadling avro :
https://www.youtube.com/watch?v=ymRfdFGv9Zg

Launch your spark-shell with avro dependencies. I had used org.apache.spark:spark-avro_2.11:2.4.4

On the exam to read avro and parquet files.
df = spark.read.format(“avro”).load(“pathtofile”).
df = spark.read.parquet(“pathtofile”)

to read parquet files with parquet-tools
parquet-tools cat --json hdfs://user/cert/q1/part-r-00000-6a3ccfae-5eb9-4a88-8ce8-b11b2644d5de.gz.parquet

to read avro files with avro-tools
avro-tools tojson hdfs://user/cert/q1/part-r-00000-6a3ccfae-5eb9-4a88-8ce8-b11b2644d5de.avro

## Handling Hive meta
https://www.youtube.com/watch?v=XNEPUwx1NmU

## Nice way to handle Questions
https://www.udemy.com/course/cca175-practice-tests-includes-a-test-environment-vm/ 



---------------------- Old formats -------------------------------------

	http://arun-teaches-u-tech.blogspot.com/p/certification-preparation-plan.html
	
	This one is more exhaustive but more indepth  -->
	https://www.alpha-epsilon.de/cca175/2017/07/16/preparing-for-the-cloudera-exam-cca175-spark-and-hadoop-developer/ 

	https://andriymz.github.io/certifications/cca175-feedback/
	
	http://nn02.itversity.com/cca175/